{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGYqVVM60gGIK6DviWDVhy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"teecvSeXqvlh","executionInfo":{"status":"ok","timestamp":1723485290824,"user_tz":420,"elapsed":329,"user":{"displayName":"Andrew Fullhart","userId":"10918403777587533818"}}},"outputs":[],"source":["%reset -f"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvDc3dTqq2_l","executionInfo":{"status":"ok","timestamp":1723485308170,"user_tz":420,"elapsed":16034,"user":{"displayName":"Andrew Fullhart","userId":"10918403777587533818"}},"outputId":"edc4622d-2677-425d-9b0b-3827c97a0221"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["The block below identifies ghcnd stations with at least some data in the 1974-2013 range, record completeness > 0%, and distance to nearest CLIGEN station up to 100000 m. Only a metadata file is created, and the records are not downloaded in this step. A file is read containing lat/lon coordinates of the U.S. CLIGEN network. Run time is ~27 min."],"metadata":{"id":"NOKn2a0f5CAc"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import requests\n","from datetime import datetime as dt\n","from datetime import timedelta\n","import os\n","import scipy as scipy\n","import statsmodels.api as sm\n","from google.colab import data_table\n","data_table.enable_dataframe_formatter()\n","import csv\n","import geopandas as gpd\n","from geopy.distance import geodesic\n","\n","\n","stationIDLINK = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n","metadataLINK = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt'\n","dataLINK = 'https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access'\n","coordFILE = '/content/drive/My Drive/Colab Notebooks/CLIGEN/US_CLIGEN_Coords.txt'\n","keepFILE = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Keepers_John.csv'\n","\n","max_dist = 100000.0 #meters\n","percent_complete = 0.0 #%\n","\n","dates = []\n","start_date = dt(1974, 1, 1, 0, 0)\n","end_date = dt(2013, 12, 31, 0, 0)\n","date = start_date\n","while date <= end_date:\n","  dates.append(date)\n","  date = date + timedelta(days=1)\n","\n","stations, longitudes, latitudes = [], [], []\n","with open(coordFILE) as f:\n","  next(f)\n","  for line in f:\n","    row = line.strip('\\n').split(',')\n","    stations.append(row[0])\n","    longitudes.append(row[1])\n","    latitudes.append(row[2])\n","\n","cli_df = pd.DataFrame(data=zip(stations, longitudes, latitudes), columns=['stationID', 'long', 'lat'])\n","\n","url = stationIDLINK\n","req = requests.get(url)\n","text = req.text\n","\n","stations, longitudes, latitudes = [], [], []\n","lines = (line for line in text.splitlines())\n","for line in lines:\n","  row = line.split()\n","  stationID = row[0]\n","  if stationID[:2] == 'US':\n","    stations.append(row[0])\n","    latitudes.append(row[1])\n","    longitudes.append(row[2])\n","\n","gnd_df = pd.DataFrame(data=zip(stations, longitudes, latitudes), columns=['stationID', 'long', 'lat'])\n","\n","#EPSG:4326 geodetic coordinates -> 'EPSG:3857' meters\n","cli_gdf = gpd.GeoDataFrame(\n","  cli_df, geometry=gpd.points_from_xy(cli_df['long'], cli_df['lat']), crs='EPSG:3857'\n",")\n","\n","gnd_gdf = gpd.GeoDataFrame(\n","  gnd_df, geometry=gpd.points_from_xy(gnd_df['long'], gnd_df['lat']), crs='EPSG:3857'\n",")\n","\n","join_gdf = gpd.sjoin_nearest(cli_gdf, gnd_gdf, how='left', distance_col='dist')\n","join_gdf.to_crs('EPSG:3857')\n","\n","join_df = pd.DataFrame(join_gdf.drop(columns=['geometry', 'index_right']))\n","join_df['dist_m'] = join_df.apply(lambda x: geodesic((x['lat_left'], x['long_left']), (x['lat_right'], x['long_right'])).meters, axis=1)\n","\n","keepers_one_df = join_df[join_df['dist_m'] < max_dist].reset_index()\n","\n","keepers_step_one = keepers_one_df['stationID_right'].values\n","\n","url = metadataLINK\n","req = requests.get(url)\n","text = req.text\n","\n","keepers_step_two = []\n","lines = (line for line in text.splitlines())\n","for line in lines:\n","  row = line.split()\n","  if 'PRCP' in row and row[0] in keepers_step_one:\n","    if int(row[4]) <= 1974 and int(row[5]) >= 2013:\n","      keepers_step_two.append(row[0])\n","\n","#No html address or too much missing data\n","bad = []\n","for keeper in keepers_step_two:\n","\n","  try:\n","    ct = 0\n","    url = dataLINK + '/' + keeper + '.csv'\n","    req = requests.get(url)\n","    text = req.text\n","    if not '404 Not Found' in text:\n","      ct = 0\n","      lines = [line for line in text.splitlines()]\n","      save_lines = []\n","      hdrs = lines[0].split(',')\n","      prcp_i = hdrs.index('\"PRCP\"')\n","      date_i = hdrs.index('\"DATE\"')\n","      for line in lines[1:]:\n","        row = line.split('\",\"')\n","        name_no_comma = row[5].replace(',', '')\n","        line = line.replace(row[5], name_no_comma)\n","        line = line.replace('\",\"', ',')\n","        row = line.split(',')\n","        date = dt.strptime(row[date_i].strip('\"'), '%Y-%m-%d')\n","        prcp = row[prcp_i].strip('\"')\n","        if date.year >= 1974 and date.year <= 2013:\n","          if prcp != '' and not any([s in prcp for s in ['P', 'T', 'H', '9999']]):\n","            prcp = float(prcp)\n","            ct += 1\n","\n","      error_per = float(ct)/float(len(dates))*100\n","      keepers_one_df.loc[keepers_one_df['stationID_right'] == keeper, 'complete_per'] = error_per\n","      if float(ct)/float(len(dates))*100. < percent_complete:\n","        pass\n","\n","      if float(ct)/float(len(dates))*100. < percent_complete:\n","        print(str(float(ct)/float(len(dates))*100))\n","        bad.append(keeper)\n","\n","    else:\n","      bad.append(keeper)\n","\n","  except requests.exceptions.Timeout:\n","    bad.append(keeper)\n","\n","\n","keepers_step_three = [k for k in keepers_step_two if k not in bad]\n","\n","keepers_df = keepers_one_df.loc[keepers_one_df['stationID_right'].isin(keepers_step_three)].reset_index()\n","\n","keepers_df.to_csv(keepFILE)\n","\n"],"metadata":{"id":"jS9ArAnHq4p3","executionInfo":{"status":"ok","timestamp":1723513734500,"user_tz":420,"elapsed":1697131,"user":{"displayName":"Andrew Fullhart","userId":"10918403777587533818"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["The block below downloads the identified records. Run time is ~6 hr."],"metadata":{"id":"KT9U5bEV6_SS"}},{"cell_type":"code","source":["from datetime import datetime as dt\n","from datetime import timedelta\n","import pandas as pd\n","import requests\n","import os\n","\n","keepFILE = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Keepers_John.csv'\n","dataLINK = 'https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access'\n","outFOLDER = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Output/GHCN_Dataframes_for_Keepers_John'\n","keepers_df = pd.read_csv(keepFILE)\n","keepers = keepers_df['stationID_right'].values\n","\n","dates = []\n","start_date = dt(1974, 1, 1, 0, 0)\n","end_date = dt(2013, 12, 31, 0, 0)\n","date = start_date\n","while date <= end_date:\n","  dates.append(date)\n","  date = date + timedelta(days=1)\n","\n","i = 0\n","done = False\n","while done == False:\n","\n","  keeper = keepers[i]\n","  print(keeper)\n","  df = pd.DataFrame()\n","  df.index.name = 'index'\n","  df['year'] = [date.year for date in dates]\n","  df['month'] = [date.month for date in dates]\n","  df['day'] = [date.day for date in dates]\n","  df[keeper] = [0.0 for date in dates]\n","\n","  outFILE = os.path.join(outFOLDER, keeper + '.csv')\n","  url = dataLINK + '/' + keeper + '.csv'\n","  req = requests.get(url)\n","  text = req.text\n","  if not '404 Not Found' in text:\n","\n","    lines = [line for line in text.splitlines()]\n","    hdrs = lines[0].split(',')\n","    prcp_col_i = hdrs.index('\"PRCP\"')\n","    date_col_i = hdrs.index('\"DATE\"')\n","\n","    for line in lines[1:]:\n","      row = line.split('\",\"')\n","      name_no_comma = row[5].replace(',', '')\n","      line = line.replace(row[5], name_no_comma)\n","      line = line.replace('\",\"', ',')\n","      row = line.split(',')\n","      lon = row[3]\n","      lat = row[2]\n","      date = dt.strptime(row[date_col_i].strip('\"'), '%Y-%m-%d')\n","      prcp = row[prcp_col_i].strip('\"')\n","\n","      if date.year >= 1974 and date.year <= 2013:\n","\n","        if prcp != '' and not any([s in prcp for s in ['P', 'T', 'H', '9999']]):\n","          prcp = float(prcp)/10.0\n","          df.loc[(df['year'] == date.year) & (df['month']==date.month) & (df['day']==date.day), keeper] = prcp\n","\n","  i += 1\n","\n","  if i == len(keepers):\n","    done = True\n","\n","  df.to_csv(outFILE)\n","\n","\n"],"metadata":{"id":"tveXvSwcxu3U"},"execution_count":null,"outputs":[]}]}