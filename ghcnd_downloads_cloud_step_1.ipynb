{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WDjH1c8GvGw"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16919,
     "status": "ok",
     "timestamp": 1721373269568,
     "user": {
      "displayName": "Andrew Fullhart",
      "userId": "10918403777587533818"
     },
     "user_tz": 420
    },
    "id": "JvTH6-UMHRil",
    "outputId": "e3eaa0b0-e697-4c5d-f404-634be1c84102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2778783,
     "status": "ok",
     "timestamp": 1720568688041,
     "user": {
      "displayName": "Andrew Fullhart",
      "userId": "10918403777587533818"
     },
     "user_tz": 420
    },
    "id": "h7zI8T6yHU_Y",
    "outputId": "914c3c1c-0df2-477b-c41f-8add20732981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.507871321013006\n",
      "27.077344284736483\n",
      "0.0\n",
      "39.31553730321697\n",
      "0.0\n",
      "34.544832306639286\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import scipy as scipy\n",
    "import statsmodels.api as sm\n",
    "from google.colab import data_table\n",
    "data_table.enable_dataframe_formatter()\n",
    "import csv\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "stationIDLINK = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "metadataLINK = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt'\n",
    "dataLINK = 'https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access'\n",
    "coordFILE = '/content/drive/My Drive/Colab Notebooks/CLIGEN/US_CLIGEN_Coords.csv'\n",
    "keepFILE = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Keepers.csv'\n",
    "\n",
    "max_dist = 100000.0 #meters\n",
    "percent_complete = 50.0 #%\n",
    "\n",
    "dates = []\n",
    "start_date = dt(1974, 1, 1, 0, 0)\n",
    "end_date = dt(2013, 12, 31, 0, 0)\n",
    "date = start_date\n",
    "while date <= end_date:\n",
    "  dates.append(date)\n",
    "  date = date + timedelta(days=1)\n",
    "\n",
    "stations, longitudes, latitudes = [], [], []\n",
    "with open(coordFILE) as f:\n",
    "  next(f)\n",
    "  for line in f:\n",
    "    row = line.strip('\\n').split(',')\n",
    "    stations.append(row[0])\n",
    "    longitudes.append(row[1])\n",
    "    latitudes.append(row[2])\n",
    "\n",
    "cli_df = pd.DataFrame(data=zip(stations, longitudes, latitudes), columns=['stationID', 'long', 'lat'])\n",
    "\n",
    "url = stationIDLINK\n",
    "req = requests.get(url)\n",
    "text = req.text\n",
    "\n",
    "stations, longitudes, latitudes = [], [], []\n",
    "lines = (line for line in text.splitlines())\n",
    "for line in lines:\n",
    "  row = line.split()\n",
    "  stationID = row[0]\n",
    "  if stationID[:2] == 'US':\n",
    "    stations.append(row[0])\n",
    "    latitudes.append(row[1])\n",
    "    longitudes.append(row[2])\n",
    "\n",
    "gnd_df = pd.DataFrame(data=zip(stations, longitudes, latitudes), columns=['stationID', 'long', 'lat'])\n",
    "\n",
    "#EPSG:4326 geodetic coordinates -> 'EPSG:3857' meters\n",
    "cli_gdf = gpd.GeoDataFrame(\n",
    "  cli_df, geometry=gpd.points_from_xy(cli_df['long'], cli_df['lat']), crs='EPSG:3857'\n",
    ")\n",
    "\n",
    "gnd_gdf = gpd.GeoDataFrame(\n",
    "  gnd_df, geometry=gpd.points_from_xy(gnd_df['long'], gnd_df['lat']), crs='EPSG:3857'\n",
    ")\n",
    "\n",
    "join_gdf = gpd.sjoin_nearest(cli_gdf, gnd_gdf, how='left', distance_col='dist')\n",
    "join_gdf.to_crs('EPSG:3857')\n",
    "\n",
    "join_df = pd.DataFrame(join_gdf.drop(columns=['geometry', 'index_right']))\n",
    "join_df['dist_m'] = join_df.apply(lambda x: geodesic((x['lat_left'], x['long_left']), (x['lat_right'], x['long_right'])).meters, axis=1)\n",
    "\n",
    "keepers_one_df = join_df[join_df['dist_m'] < max_dist].reset_index()\n",
    "\n",
    "keepers_step_one = keepers_one_df['stationID_right'].values\n",
    "\n",
    "url = metadataLINK\n",
    "req = requests.get(url)\n",
    "text = req.text\n",
    "\n",
    "keepers_step_two = []\n",
    "lines = (line for line in text.splitlines())\n",
    "for line in lines:\n",
    "  row = line.split()\n",
    "  if 'PRCP' in row and row[0] in keepers_step_one:\n",
    "    if int(row[4]) <= 1974 and int(row[5]) >= 2013:\n",
    "      keepers_step_two.append(row[0])\n",
    "\n",
    "#No html address\n",
    "bad = []\n",
    "for keeper in keepers_step_two:\n",
    "\n",
    "  try:\n",
    "    ct = 0\n",
    "    url = dataLINK + '/' + keeper + '.csv'\n",
    "    req = requests.get(url)\n",
    "    text = req.text\n",
    "    if not '404 Not Found' in text:\n",
    "      ct = 0\n",
    "      lines = [line for line in text.splitlines()]\n",
    "      save_lines = []\n",
    "      hdrs = lines[0].split(',')\n",
    "      prcp_i = hdrs.index('\"PRCP\"')\n",
    "      date_i = hdrs.index('\"DATE\"')\n",
    "      for line in lines[1:]:\n",
    "        row = line.split('\",\"')\n",
    "        name_no_comma = row[5].replace(',', '')\n",
    "        line = line.replace(row[5], name_no_comma)\n",
    "        line = line.replace('\",\"', ',')\n",
    "        row = line.split(',')\n",
    "        date = dt.strptime(row[date_i].strip('\"'), '%Y-%m-%d')\n",
    "        prcp = row[prcp_i].strip('\"')\n",
    "        if date.year >= 1974 and date.year <= 2013:\n",
    "          if prcp != '' and not any([s in prcp for s in ['P', 'T', 'H', '9999']]):\n",
    "            prcp = float(prcp)\n",
    "            ct += 1\n",
    "\n",
    "      if float(ct)/float(len(dates))*100. < percent_complete:\n",
    "        print(str(float(ct)/float(len(dates))*100))\n",
    "        bad.append(keeper)\n",
    "\n",
    "    else:\n",
    "      bad.append(keeper)\n",
    "\n",
    "  except requests.exceptions.Timeout:\n",
    "    pass\n",
    "\n",
    "\n",
    "keepers_step_three = [k for k in keepers_step_two if k not in bad]\n",
    "\n",
    "keepers_df = keepers_one_df.loc[keepers_one_df['stationID_right'].isin(keepers_step_three)].reset_index()\n",
    "\n",
    "keepers_df.to_csv(keepFILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCtNtbrzdbix"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "keepFILE = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Keepers.csv'\n",
    "dataLINK = 'https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access'\n",
    "outFOLDER = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Output/GHCN_Dataframe_for_Keepers_95'\n",
    "keepers_df = pd.read_csv(keepFILE)\n",
    "keepers = keepers_df['stationID_right'].values\n",
    "\n",
    "dates = []\n",
    "start_date = dt(1974, 1, 1, 0, 0)\n",
    "end_date = dt(2013, 12, 31, 0, 0)\n",
    "date = start_date\n",
    "while date <= end_date:\n",
    "  dates.append(date)\n",
    "  date = date + timedelta(days=1)\n",
    "\n",
    "i = 0\n",
    "done = False\n",
    "while done == False:\n",
    "\n",
    "  keeper = keepers[i]\n",
    "  print(keeper)\n",
    "  df = pd.DataFrame()\n",
    "  df.index.name = 'index'\n",
    "  df['year'] = [date.year for date in dates]\n",
    "  df['month'] = [date.month for date in dates]\n",
    "  df['day'] = [date.day for date in dates]\n",
    "  df[keeper] = [0.0 for date in dates]\n",
    "\n",
    "  outFILE = os.path.join(outFOLDER, keeper + '.csv')\n",
    "  url = dataLINK + '/' + keeper + '.csv'\n",
    "  req = requests.get(url)\n",
    "  text = req.text\n",
    "  if not '404 Not Found' in text:\n",
    "\n",
    "    lines = [line for line in text.splitlines()]\n",
    "    hdrs = lines[0].split(',')\n",
    "    prcp_col_i = hdrs.index('\"PRCP\"')\n",
    "    date_col_i = hdrs.index('\"DATE\"')\n",
    "\n",
    "    for line in lines[1:]:\n",
    "      row = line.split('\",\"')\n",
    "      name_no_comma = row[5].replace(',', '')\n",
    "      line = line.replace(row[5], name_no_comma)\n",
    "      line = line.replace('\",\"', ',')\n",
    "      row = line.split(',')\n",
    "      lon = row[3]\n",
    "      lat = row[2]\n",
    "      date = dt.strptime(row[date_col_i].strip('\"'), '%Y-%m-%d')\n",
    "      prcp = row[prcp_col_i].strip('\"')\n",
    "\n",
    "      if date.year >= 1974 and date.year <= 2013:\n",
    "\n",
    "        if prcp != '' and not any([s in prcp for s in ['P', 'T', 'H', '9999']]):\n",
    "          prcp = float(prcp)/10.0\n",
    "          df.loc[(df['year'] == date.year) & (df['month']==date.month) & (df['day']==date.day), keeper] = prcp\n",
    "\n",
    "  i += 1\n",
    "\n",
    "  if i == len(keepers):\n",
    "    done = True\n",
    "\n",
    "  df.to_csv(outFILE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244958,
     "status": "ok",
     "timestamp": 1720901236551,
     "user": {
      "displayName": "Andrew Fullhart",
      "userId": "10918403777587533818"
     },
     "user_tz": 420
    },
    "id": "wBy-9fIZuk9K",
    "outputId": "5a62f694-506a-4329-8693-d0c3c3571c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_St. Clair Co IL_CLIGEN.cli': 'il110510.par', '11_Washington Co AR_CLIGEN.cli': 'ar032443.par', '12_Crosby Co TX_CLIGEN.cli': 'tx412121.par', '13_Pratt Co KS_CLIGEN.cli': 'ks146549.par', '14_Washington Co CO_CLIGEN.cli': 'co050114.par', '15_Prairie Co MT_CLIGEN.cli': 'mt248169.par', '16_Maricopa Co AZ_CLIGEN.cli': 'az024829.par', '17_Fresno Co CA_CLIGEN.cli': 'ca040449.par', '18_Jerome Co ID_CLIGEN.cli': 'id104670.par', '19_Adams Co WA_CLIGEN.cli': 'wa454679.par', '1_Steuben Co NY_CLIGEN.cli': 'ny300448.par', '20_Linn Co OR_CLIGEN.cli': 'or354811.par', '2_Randolf Co NC_CLIGEN.cli': 'nc310286.par', '3_Jackson Co FL_CLIGEN.cli': 'fl081544.par', '4_Putnam Co OH_CLIGEN.cli': 'oh336405.par', '5_Bolivar Co MS_CLIGEN.cli': 'ms221707.par', '6_Adair Co IA_CLIGEN.cli': 'ia133438.par', '7_Sauk Co WI_CLIGEN.cli': 'wi470516.par', '8_McLean Co ND_CLIGEN.cli': 'nd328872.par', '9_Kearney Co NE_CLIGEN.cli': 'ne255565.par'}\n",
      "20\n",
      "      Unnamed: 0  level_0  index stationID_left   long_left   lat_left  \\\n",
      "0              0        0      0       al010252  -86.500001  31.319999   \n",
      "1              1        1      1       al010272  -85.849998  33.580001   \n",
      "2              2        3      4       al010583  -87.779999  30.879997   \n",
      "3              3        5      6       al010831  -86.750001  33.569999   \n",
      "4              4        6      7       al011084  -87.050004  31.069999   \n",
      "...          ...      ...    ...            ...         ...        ...   \n",
      "1179        1179     2569   2687       wy487200 -104.480002  41.630002   \n",
      "1180        1180     2571   2689       wy487533 -107.199995  41.800001   \n",
      "1181        1181     2572   2690       wy487555 -104.279998  43.249998   \n",
      "1182        1182     2576   2694       wy488160 -106.870004  44.850000   \n",
      "1183        1183     2580   2698       wy489205 -104.620004  44.099997   \n",
      "\n",
      "     stationID_right  long_right  lat_right      dist       dist_m  \n",
      "0        USC00010252    -86.5225    31.3072  0.025885  2569.159149  \n",
      "1        USW00013871    -85.8478    33.5906  0.010825  1193.195908  \n",
      "2        USC00010583    -87.7853    30.8839  0.006582   666.425279  \n",
      "3        USW00013876    -86.7450    33.5656  0.006660   673.521948  \n",
      "4        USC00011084    -87.0550    31.0583  0.012721  1381.973914  \n",
      "...              ...         ...        ...       ...          ...  \n",
      "1179     USC00487200   -104.4936    41.6264  0.014067  1201.723726  \n",
      "1180     USW00024057   -107.1953    41.8064  0.007937   810.787207  \n",
      "1181     USC00487555   -104.2881    43.2450  0.009520   861.011097  \n",
      "1182     USC00488160   -106.8383    44.8406  0.033068  2715.434597  \n",
      "1183     USC00489205   -104.6114    44.0928  0.011217  1055.527045  \n",
      "\n",
      "[1184 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "keepFILE = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Keepers95.csv'\n",
    "parFOLDER = '/content/drive/My Drive/Colab Notebooks/CLIGEN/2015parfiles/2015parfiles/2015parfiles'\n",
    "anuragFOLDER = '/content/drive/My Drive/Colab Notebooks/TeamProjects/CLIGEN'\n",
    "dataFOLDER = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Output/GHCN_Dataframes_for_Keepers_95'\n",
    "shareFOLDER = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Share'\n",
    "\n",
    "keep_df = pd.read_csv(keepFILE)\n",
    "\n",
    "gnd_stations = keep_df['stationID_right'].values\n",
    "\n",
    "anuragFILES = os.listdir(anuragFOLDER)\n",
    "parFILES = [f for f in os.listdir(parFOLDER) if f != 'stations2015.txt' ]\n",
    "matches_dct = {}\n",
    "for anuragf in anuragFILES:\n",
    "  with open(os.path.join(anuragFOLDER, anuragf)) as f:\n",
    "    next(f); next(f); next(f); next(f)\n",
    "    line= f.readline()\n",
    "    row = line.split()\n",
    "    latitude = row[0]\n",
    "    longitude = row[1]\n",
    "\n",
    "  matches_dct[anuragf] = None\n",
    "\n",
    "  for parf in parFILES:\n",
    "    with open(os.path.join(parFOLDER, parf)) as f:\n",
    "      next(f)\n",
    "      line = f.readline()\n",
    "      row = line.split('=')\n",
    "      lat = row[1].split()[0]\n",
    "      lon = row[2].split()[0]\n",
    "\n",
    "    if latitude == lat and longitude == lon:\n",
    "      matches_dct[anuragf] = parf\n",
    "      break\n",
    "\n",
    "\n",
    "print(matches_dct)\n",
    "print(len(matches_dct))\n",
    "\n",
    "\n",
    "print(keep_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1720902904747,
     "user": {
      "displayName": "Andrew Fullhart",
      "userId": "10918403777587533818"
     },
     "user_tz": 420
    },
    "id": "2XQQRpdmb-Wr",
    "outputId": "94f39bb7-5716-4e83-883f-b3a8169e8e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_St. Clair Co IL_CLIGEN.cli': 'il110510.par', '11_Washington Co AR_CLIGEN.cli': 'ar032443.par', '12_Crosby Co TX_CLIGEN.cli': 'tx412121.par', '13_Pratt Co KS_CLIGEN.cli': 'ks146549.par', '14_Washington Co CO_CLIGEN.cli': 'co050114.par', '15_Prairie Co MT_CLIGEN.cli': 'mt248169.par', '16_Maricopa Co AZ_CLIGEN.cli': 'az024829.par', '17_Fresno Co CA_CLIGEN.cli': 'ca040449.par', '18_Jerome Co ID_CLIGEN.cli': 'id104670.par', '19_Adams Co WA_CLIGEN.cli': 'wa454679.par', '1_Steuben Co NY_CLIGEN.cli': 'ny300448.par', '20_Linn Co OR_CLIGEN.cli': 'or354811.par', '2_Randolf Co NC_CLIGEN.cli': 'nc310286.par', '3_Jackson Co FL_CLIGEN.cli': 'fl081544.par', '4_Putnam Co OH_CLIGEN.cli': 'oh336405.par', '5_Bolivar Co MS_CLIGEN.cli': 'ms221707.par', '6_Adair Co IA_CLIGEN.cli': 'ia133438.par', '7_Sauk Co WI_CLIGEN.cli': 'wi470516.par', '8_McLean Co ND_CLIGEN.cli': 'nd328872.par', '9_Kearney Co NE_CLIGEN.cli': 'ne255565.par'}\n",
      "il110510\n",
      "[]\n",
      "ar032443\n",
      "['USW00093993']\n",
      "tx412121\n",
      "[]\n",
      "ks146549\n",
      "[]\n",
      "co050114\n",
      "[]\n",
      "mt248169\n",
      "[]\n",
      "az024829\n",
      "[]\n",
      "ca040449\n",
      "['USC00040449']\n",
      "id104670\n",
      "['USC00104670']\n",
      "wa454679\n",
      "['USC00454679']\n",
      "ny300448\n",
      "[]\n",
      "or354811\n",
      "['USC00354811']\n",
      "nc310286\n",
      "[]\n",
      "fl081544\n",
      "[]\n",
      "oh336405\n",
      "['USC00336405']\n",
      "ms221707\n",
      "['USC00221707']\n",
      "ia133438\n",
      "['USC00133438']\n",
      "wi470516\n",
      "['USC00470516']\n",
      "nd328872\n",
      "[]\n",
      "ne255565\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#None in matches_dct because...\n",
    "#No GHCNd/CLIGEN match because...\n",
    "\n",
    "print(matches_dct)\n",
    "\n",
    "for key in matches_dct:\n",
    "\n",
    "  if matches_dct[key] != None:\n",
    "    print(matches_dct[key][:-4])\n",
    "    print(keep_df.loc[keep_df['stationID_left'] == matches_dct[key][:-4], 'stationID_right'].values)\n",
    "\n",
    "\n",
    "\n",
    "#inpath = os.path.join(dataFOLDER, keep_df['stationID_right'].loc[keep_df['stationID_left'] == matches_dct[key]])\n",
    "#outpath = os.path.join(shareFOLDER, )\n",
    "\n",
    "outFILE = '/content/drive/My Drive/Colab Notebooks/TeamProjects/TwentyFourStations.csv'\n",
    "extrapars_list = ['nv264436', 'ut425733', 'az028619', 'nm290234']\n",
    "\n",
    "#print list of closest par stations to ghcn stations:\n",
    "with open(outFILE, 'w') as fo:\n",
    "  fo.write('stationID,x,y\\n')\n",
    "  for key in matches_dct:\n",
    "    par = matches_dct[key]\n",
    "    with open(os.path.join(parFOLDER, par)) as parf:\n",
    "      next(parf)\n",
    "      line = parf.readline()\n",
    "      row = line.split('=')\n",
    "      lat = row[1].split()[0]\n",
    "      lon = row[2].split()[0]\n",
    "    fo.write(','.join([par[:-4], lon, lat]) + '\\n')\n",
    "\n",
    "  for par in extrapars_list:\n",
    "    par = par + '.par'\n",
    "    with open(os.path.join(parFOLDER, par)) as parf:\n",
    "      next(parf)\n",
    "      line = parf.readline()\n",
    "      row = line.split('=')\n",
    "      lat = row[1].split()[0]\n",
    "      lon = row[2].split()[0]\n",
    "    fo.write(','.join([par[:-4], lon, lat]) + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWOCbY8Xtb3a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import scipy as scipy\n",
    "import statsmodels.api as sm\n",
    "from google.colab import data_table\n",
    "data_table.enable_dataframe_formatter()\n",
    "import csv\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "stationIDLINK = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "metadataLINK = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt'\n",
    "dataLINK = 'https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access'\n",
    "coordFILE = '/content/drive/My Drive/Colab Notebooks/TeamProjects/TwentyFourStations.csv'\n",
    "keepFILE = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Keepers.csv'\n",
    "\n",
    "max_dist = 100000.0 #meters\n",
    "percent_complete = 95.0 #%\n",
    "\n",
    "dates = []\n",
    "start_date = dt(1974, 1, 1, 0, 0)\n",
    "end_date = dt(2013, 12, 31, 0, 0)\n",
    "date = start_date\n",
    "while date <= end_date:\n",
    "  dates.append(date)\n",
    "  date = date + timedelta(days=1)\n",
    "\n",
    "stations, longitudes, latitudes = [], [], []\n",
    "with open(coordFILE) as f:\n",
    "  next(f)\n",
    "  for line in f:\n",
    "    row = line.strip('\\n').split(',')\n",
    "    stations.append(row[0])\n",
    "    longitudes.append(row[1])\n",
    "    latitudes.append(row[2])\n",
    "\n",
    "cli_df = pd.DataFrame(data=zip(stations, longitudes, latitudes), columns=['stationID', 'long', 'lat'])\n",
    "\n",
    "url = stationIDLINK\n",
    "req = requests.get(url)\n",
    "text = req.text\n",
    "\n",
    "stations, longitudes, latitudes = [], [], []\n",
    "lines = (line for line in text.splitlines())\n",
    "for line in lines:\n",
    "  row = line.split()\n",
    "  stationID = row[0]\n",
    "  if stationID[:2] == 'US':\n",
    "    stations.append(row[0])\n",
    "    latitudes.append(row[1])\n",
    "    longitudes.append(row[2])\n",
    "\n",
    "gnd_df = pd.DataFrame(data=zip(stations, longitudes, latitudes), columns=['stationID', 'long', 'lat'])\n",
    "\n",
    "#EPSG:4326 geodetic coordinates -> 'EPSG:3857' meters\n",
    "cli_gdf = gpd.GeoDataFrame(\n",
    "  cli_df, geometry=gpd.points_from_xy(cli_df['long'], cli_df['lat']), crs='EPSG:3857'\n",
    ")\n",
    "\n",
    "gnd_gdf = gpd.GeoDataFrame(\n",
    "  gnd_df, geometry=gpd.points_from_xy(gnd_df['long'], gnd_df['lat']), crs='EPSG:3857'\n",
    ")\n",
    "\n",
    "join_gdf = gpd.sjoin_nearest(cli_gdf, gnd_gdf, how='left', distance_col='dist')\n",
    "join_gdf.to_crs('EPSG:3857')\n",
    "\n",
    "join_df = pd.DataFrame(join_gdf.drop(columns=['geometry', 'index_right']))\n",
    "join_df['dist_m'] = join_df.apply(lambda x: geodesic((x['lat_left'], x['long_left']), (x['lat_right'], x['long_right'])).meters, axis=1)\n",
    "\n",
    "keepers_one_df = join_df[join_df['dist_m'] < max_dist].reset_index()\n",
    "\n",
    "keepers_step_one = keepers_one_df['stationID_right'].values\n",
    "\n",
    "url = metadataLINK\n",
    "req = requests.get(url)\n",
    "text = req.text\n",
    "\n",
    "\"\"\"\n",
    "keepers_step_two = []\n",
    "lines = (line for line in text.splitlines())\n",
    "for line in lines:\n",
    "  row = line.split()\n",
    "  if 'PRCP' in row and row[0] in keepers_step_one:\n",
    "    if int(row[4]) <= 1974 and int(row[5]) >= 2013:\n",
    "      keepers_step_two.append(row[0])\n",
    "\n",
    "print('length keepers_step_two: ', len(keepers_step_two))\n",
    "\"\"\"\n",
    "\n",
    "keepers_step_two = keepers_step_one\n",
    "\n",
    "#No html address\n",
    "bad = []\n",
    "for keeper in keepers_step_two:\n",
    "\n",
    "  ct = 0\n",
    "  url = dataLINK + '/' + keeper + '.csv'\n",
    "  req = requests.get(url)\n",
    "  text = req.text\n",
    "\n",
    "  lines = [line for line in text.splitlines()]\n",
    "\n",
    "  save_lines = []\n",
    "  hdrs = lines[0].split(',')\n",
    "  prcp_i = hdrs.index('\"PRCP\"')\n",
    "  date_i = hdrs.index('\"DATE\"')\n",
    "  for line in lines[1:]:\n",
    "    row = line.split('\",\"')\n",
    "    name_no_comma = row[5].replace(',', '')\n",
    "    line = line.replace(row[5], name_no_comma)\n",
    "    line = line.replace('\",\"', ',')\n",
    "    row = line.split(',')\n",
    "    date = dt.strptime(row[date_i].strip('\"'), '%Y-%m-%d')\n",
    "    prcp = row[prcp_i].strip('\"')\n",
    "    if date.year >= 1974 and date.year <= 2013:\n",
    "      if prcp != '' and not any([s in prcp for s in ['P', 'T', 'H', '9999']]):\n",
    "        prcp = float(prcp)\n",
    "        ct += 1\n",
    "\n",
    "  error_per = float(ct)/float(len(dates))*100\n",
    "  join_df.loc[join_df['stationID_right'] == keeper, 'complete_per'] = error_per\n",
    "  if float(ct)/float(len(dates))*100. < percent_complete:\n",
    "    pass\n",
    "\n",
    "join_df.index.name = 'index'\n",
    "join_df.to_csv('/content/drive/My Drive/Colab Notebooks/TeamProjects/read_out.csv')\n",
    "\n",
    "\n",
    "dataLINK = 'https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access'\n",
    "outFOLDER = '/content/drive/My Drive/Colab Notebooks/TeamProjects/Share'\n",
    "for row in join_df.loc[join_df['complete_per'] >= 95.0].iterrows():\n",
    "\n",
    "  keeper = row[1]['stationID_right']\n",
    "  dates = []\n",
    "  start_date = dt(1974, 1, 1, 0, 0)\n",
    "  end_date = dt(2013, 12, 31, 0, 0)\n",
    "  date = start_date\n",
    "  while date <= end_date:\n",
    "    dates.append(date)\n",
    "    date = date + timedelta(days=1)\n",
    "\n",
    "  df = pd.DataFrame()\n",
    "  df.index.name = 'index'\n",
    "  df['year'] = [date.year for date in dates]\n",
    "  df['month'] = [date.month for date in dates]\n",
    "  df['day'] = [date.day for date in dates]\n",
    "  df[keeper] = [0.0 for date in dates]\n",
    "\n",
    "  outFILE = os.path.join(outFOLDER, keeper + '.csv')\n",
    "  url = dataLINK + '/' + keeper + '.csv'\n",
    "  req = requests.get(url)\n",
    "  text = req.text\n",
    "  if not '404 Not Found' in text:\n",
    "\n",
    "    lines = [line for line in text.splitlines()]\n",
    "    hdrs = lines[0].split(',')\n",
    "    prcp_col_i = hdrs.index('\"PRCP\"')\n",
    "    date_col_i = hdrs.index('\"DATE\"')\n",
    "\n",
    "    for line in lines[1:]:\n",
    "      row = line.split('\",\"')\n",
    "      name_no_comma = row[5].replace(',', '')\n",
    "      line = line.replace(row[5], name_no_comma)\n",
    "      line = line.replace('\",\"', ',')\n",
    "      row = line.split(',')\n",
    "      lon = row[3]\n",
    "      lat = row[2]\n",
    "      date = dt.strptime(row[date_col_i].strip('\"'), '%Y-%m-%d')\n",
    "      prcp = row[prcp_col_i].strip('\"')\n",
    "\n",
    "      if date.year >= 1974 and date.year <= 2013:\n",
    "\n",
    "        if prcp != '' and not any([s in prcp for s in ['P', 'T', 'H', '9999']]):\n",
    "          prcp = float(prcp)/10.0\n",
    "          df.loc[(df['year'] == date.year) & (df['month']==date.month) & (df['day']==date.day), keeper] = prcp\n",
    "\n",
    "  df.to_csv(outFILE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfhuNvmeLr8M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyONR1ltdBRnQWNnAFxfoaHw",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
